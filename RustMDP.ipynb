{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZR3gLCsmU8viMKCj5s9Qu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nosadchiy/public/blob/main/RustMDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2g0xf0RxbsBB",
        "outputId": "498106ef-69b3-42db-cc3a-c4476bd9dc46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimation Results:\n",
            "  Success: True\n",
            "  Estimated parameters: [26.2685504   1.72883523]\n",
            "  Negative log-likelihood: 415.2625654922745\n",
            "  Message: CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH\n"
          ]
        }
      ],
      "source": [
        "#!pip install ipdb\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import minimize\n",
        "#import ipdb\n",
        "\n",
        "##############################\n",
        "# 1. Define the Model\n",
        "##############################\n",
        "\n",
        "class ReplacementMDP:\n",
        "    \"\"\"\n",
        "    A toy replacement model:\n",
        "    - States: s in {0, 1, 2, ..., S-1} can represent \"wear level\" of a machine/bus engine.\n",
        "    - Actions: 0 = 'Continue', 1 = 'Replace'.\n",
        "    - Transition probabilities:\n",
        "        If action=Continue, wear state goes up by 1 with some probability\n",
        "        (or transitions to a terminal wear state).\n",
        "        If action=Replace, wear state goes back to 0 with probability 1.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 S,             # Number of states\n",
        "                 beta,          # Discount factor (could be estimated)\n",
        "                 cost_replace,  # Replacement cost (to be estimated)\n",
        "                 cost_usage     # Usage cost per \"wear state\" (to be estimated)\n",
        "                 ):\n",
        "        self.S = S\n",
        "        self.beta = beta\n",
        "        self.cost_replace = cost_replace\n",
        "        self.cost_usage = cost_usage\n",
        "\n",
        "    def rewards(self, s, a):\n",
        "        \"\"\"\n",
        "        Reward function r(s,a;theta).\n",
        "        We'll treat it as negative costs:\n",
        "          - If we continue (a=0), cost = cost_usage * s\n",
        "          - If we replace (a=1), cost = cost_replace\n",
        "        Return negative cost as \"reward\".\n",
        "        \"\"\"\n",
        "        if a == 0:\n",
        "            return - self.cost_usage * s\n",
        "        else:\n",
        "            return - self.cost_replace\n",
        "\n",
        "    def transition_probs(self, s, a):\n",
        "        \"\"\"\n",
        "        Transition probabilities P(s'|s,a).\n",
        "        For simplicity:\n",
        "          - If a=0 (Continue), then:\n",
        "                s' = min(s+1, S-1) with prob=1\n",
        "          - If a=1 (Replace), then:\n",
        "                s' = 0 with prob=1\n",
        "        \"\"\"\n",
        "        next_state = np.zeros(self.S)\n",
        "        if a == 0:\n",
        "            next_s = min(s+1, self.S-1)\n",
        "            next_state[next_s] = 1.0\n",
        "        else:\n",
        "            # replace -> go to state=0\n",
        "            next_state[0] = 1.0\n",
        "        #ipdb.set_trace()\n",
        "        return next_state\n",
        "\n",
        "##############################\n",
        "# 2. Solve the Dynamic Program\n",
        "##############################\n",
        "\n",
        "def solve_value_function(mdp, tol=1e-8, max_iter=10000):\n",
        "    \"\"\"\n",
        "    Value iteration to solve for V(s).\n",
        "    Returns: V[s] and choice-specific value functions Q[s,a].\n",
        "    \"\"\"\n",
        "    S = mdp.S\n",
        "    beta = mdp.beta\n",
        "\n",
        "    # Initialize value function\n",
        "    V = np.zeros(S)\n",
        "\n",
        "    for it in range(max_iter):\n",
        "        V_old = V.copy()\n",
        "\n",
        "        # Compute choice-specific values Q(s,a)\n",
        "        Q = np.zeros((S, 2))\n",
        "        for s in range(S):\n",
        "            for a in [0, 1]:\n",
        "                r_sa = mdp.rewards(s, a)\n",
        "                P_sa = mdp.transition_probs(s, a)\n",
        "                Q[s, a] = r_sa + beta * np.sum(P_sa * V_old)\n",
        "\n",
        "        # Update the value function with max over actions\n",
        "        V = np.max(Q, axis=1)\n",
        "\n",
        "        # Check convergence\n",
        "        if np.max(np.abs(V - V_old)) < tol:\n",
        "            break\n",
        "\n",
        "    return V, Q\n",
        "\n",
        "\n",
        "##############################\n",
        "# 3. Choice Probabilities\n",
        "##############################\n",
        "\n",
        "def choice_probabilities(Q, mu=1.0):\n",
        "    \"\"\"\n",
        "    Suppose the agent chooses actions with logit probabilities:\n",
        "      P(a|s) = exp( Q[s,a]/mu ) / sum_{a'} exp( Q[s,a']/mu ).\n",
        "    mu is the \"scale\" of the Type I Extreme Value error.\n",
        "    Return a matrix of shape (S, 2), where row s is [P(a=0|s), P(a=1|s)].\n",
        "    \"\"\"\n",
        "    # Q is shape (S, 2)\n",
        "    exp_Q = np.exp(Q / mu)\n",
        "    denom = np.sum(exp_Q, axis=1, keepdims=True)\n",
        "    P = exp_Q / denom\n",
        "    return P\n",
        "\n",
        "##############################\n",
        "# 4. Log-Likelihood Function\n",
        "##############################\n",
        "\n",
        "def log_likelihood(theta, data, S, beta=0.95, mu=1.0):\n",
        "    \"\"\"\n",
        "    theta: array of parameters [cost_replace, cost_usage].\n",
        "    data:  list of (s, a) observations from actual decisions.\n",
        "    Returns negative log-likelihood (for minimization).\n",
        "\n",
        "    The function:\n",
        "        1) Construct the MDP with given theta.\n",
        "        2) Solve for the value function Q(s,a).\n",
        "        3) Compute P(a|s).\n",
        "        4) Evaluate the log-likelihood of observed data under that policy.\n",
        "    \"\"\"\n",
        "    cost_replace, cost_usage = theta\n",
        "    # Bound the parameters to avoid negative or meaningless values, if needed\n",
        "    if cost_replace <= 0 or cost_usage < 0:\n",
        "        return 1e8  # penalize invalid parameters\n",
        "\n",
        "    # Construct the MDP with these parameter values\n",
        "    mdp = ReplacementMDP(S=S, beta=beta, cost_replace=cost_replace, cost_usage=cost_usage)\n",
        "\n",
        "    # Solve for the value function and Q\n",
        "    _, Q = solve_value_function(mdp)\n",
        "\n",
        "    # Compute choice probabilities\n",
        "    P = choice_probabilities(Q, mu=mu)\n",
        "\n",
        "    # Evaluate likelihood\n",
        "    ll = 0.0\n",
        "    for (s_obs, a_obs) in data:\n",
        "        ll += np.log(P[s_obs, a_obs] + 1e-12)  # +1e-12 to avoid log(0)\n",
        "\n",
        "    return -ll  # Return negative log-likelihood for minimization\n",
        "\n",
        "##############################\n",
        "# 5. Example \"Estimation\"\n",
        "##############################\n",
        "\n",
        "def simulate_data(mdp, n=1000, seed=42, mu=1.0):\n",
        "    \"\"\"\n",
        "    Generate synthetic data from the MDP under logit choice.\n",
        "    We assume each period the agent is in some state s,\n",
        "    chooses a with probability P(a|s), then moves to next state.\n",
        "    \"\"\"\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Solve for V, Q\n",
        "    _, Q = solve_value_function(mdp)\n",
        "    # Compute choice probabilities\n",
        "    P = choice_probabilities(Q, mu=mu)\n",
        "\n",
        "    # Simulate states and actions\n",
        "    data = []\n",
        "    s = 0  # start from state 0 for simplicity\n",
        "    for t in range(n):\n",
        "        # draw action from P(a|s)\n",
        "        a = np.random.choice([0,1], p=P[s, :])\n",
        "        data.append((s, a))\n",
        "\n",
        "        # transition\n",
        "        trans = mdp.transition_probs(s, a)\n",
        "        s_next = np.random.choice(mdp.S, p=trans)\n",
        "        s = s_next\n",
        "\n",
        "    return data\n",
        "\n",
        "def estimate_parameters(data, S, beta, mu=1.0):\n",
        "    \"\"\"\n",
        "    Estimate parameters using SciPy's minimize to maximize likelihood.\n",
        "    We'll do a simple unconstrained search for [cost_replace, cost_usage].\n",
        "    \"\"\"\n",
        "    # Objective function\n",
        "    def objective(theta):\n",
        "        return log_likelihood(theta, data, S, beta=beta, mu=mu)\n",
        "\n",
        "    # Initial guess\n",
        "    theta0 = np.array([5.0, 1.0])  # e.g., [cost_replace=5, cost_usage=1]\n",
        "\n",
        "    # We can set bounds if needed, for example:\n",
        "    bnds = [(1e-3, None), (0, None)]  # cost_replace>0, cost_usage>=0\n",
        "\n",
        "    result = minimize(objective, theta0, method='L-BFGS-B', bounds=bnds)\n",
        "    return result\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # True parameters for data generation\n",
        "    true_cost_replace = 30.0\n",
        "    true_cost_usage = 2.0\n",
        "    true_beta = 0.95\n",
        "    true_S = 10 # max state\n",
        "    mu = 1  # scale parameter for logit errors (not estimated in this example)\n",
        "\n",
        "    # Construct MDP with true parameters\n",
        "    true_mdp = ReplacementMDP(S=true_S, beta=true_beta,\n",
        "                              cost_replace=true_cost_replace,\n",
        "                              cost_usage=true_cost_usage)\n",
        "\n",
        "    # Simulate data\n",
        "    data = simulate_data(true_mdp, n=2000, seed=123, mu=mu)\n",
        "\n",
        "    # Estimate parameters\n",
        "    est_result = estimate_parameters(data, S=true_S, beta=true_beta, mu=mu)\n",
        "    print(\"Estimation Results:\")\n",
        "    print(\"  Success:\", est_result.success)\n",
        "    print(\"  Estimated parameters:\", est_result.x)\n",
        "    print(\"  Negative log-likelihood:\", est_result.fun)\n",
        "    print(\"  Message:\", est_result.message)\n"
      ]
    }
  ]
}